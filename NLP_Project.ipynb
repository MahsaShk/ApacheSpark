{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "## Spam detection\n",
    "\n",
    "Design an SMS Spam detection using spark NLP tools and a Naive Bayes classifier.\n",
    "\n",
    "## Dataset \n",
    "\n",
    "UCI Repository SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "The dataset contains one set of SMS messages in English inclusing 5,574 messages that are tagged as being ham (legitimate) or spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlpProject').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"smsspamcollection/SMSSpamCollection\",inferSchema=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| tag|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.withColumnRenamed('_c0', 'tag').withColumnRenamed('_c1', 'message')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "| tag|             message|               words|\n",
      "+----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar, joking,...|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|\n",
      "| ham|Nah I don't think...|[nah, i, don, t, ...|\n",
      "|spam|FreeMsg Hey there...|[freemsg, hey, th...|\n",
      "| ham|Even my brother i...|[even, my, brothe...|\n",
      "| ham|As per your reque...|[as, per, your, r...|\n",
      "|spam|WINNER!! As a val...|[winner, as, a, v...|\n",
      "|spam|Had your mobile 1...|[had, your, mobil...|\n",
      "| ham|I'm gonna be home...|[i, m, gonna, be,...|\n",
      "|spam|SIX chances to wi...|[six, chances, to...|\n",
      "|spam|URGENT! You have ...|[urgent, you, hav...|\n",
      "| ham|I've been searchi...|[i, ve, been, sea...|\n",
      "| ham|I HAVE A DATE ON ...|[i, have, a, date...|\n",
      "|spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|\n",
      "| ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|\n",
      "| ham|Eh u remember how...|[eh, u, remember,...|\n",
      "| ham|Fine if thats th...|[fine, if, that, ...|\n",
      "|spam|England v Macedon...|[england, v, mace...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing on the message column\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\w+\", gaps= False)\n",
    "df_tokenized = regexTokenizer.transform(df)\n",
    "df_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "| tag|             message|               words|             removed|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "| ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|\n",
      "|spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "| ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|\n",
      "| ham|As per your reque...|[as, per, your, r...|[per, request, me...|\n",
      "|spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|\n",
      "|spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|\n",
      "| ham|I'm gonna be home...|[i, m, gonna, be,...|[m, gonna, home, ...|\n",
      "|spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|\n",
      "|spam|URGENT! You have ...|[urgent, you, hav...|[urgent, won, 1, ...|\n",
      "| ham|I've been searchi...|[i, ve, been, sea...|[ve, searching, r...|\n",
      "| ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|\n",
      "|spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "| ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|[oh, k, m, watching]|\n",
      "| ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|\n",
      "| ham|Fine if thats th...|[fine, if, that, ...|[fine, way, u, fe...|\n",
      "|spam|England v Macedon...|[england, v, mace...|[england, v, mace...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#StopWord removing on message column\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol ='words', outputCol ='removed')\n",
    "df_tokenized_stw = remover.transform(df_tokenized)\n",
    "df_tokenized_stw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TF-IDF feature vector\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "# 1st step: get TF (term frequency) \n",
    "hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures') \n",
    "featurized_data = hashing_tf.transform(df_tokenized_stw)\n",
    "\n",
    "# 2nd step: get IDF\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| tag|             message|               words|             removed|         rawFeatures|            features|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(262144,[38555,52...|(262144,[38555,52...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|(262144,[16877,51...|(262144,[16877,51...|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(262144,[12250,12...|(262144,[12250,12...|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(262144,[2306,517...|(262144,[2306,517...|\n",
      "| ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|(262144,[19036,25...|(262144,[19036,25...|\n",
      "|spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(262144,[19036,19...|(262144,[19036,19...|\n",
      "| ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|(262144,[27576,48...|(262144,[27576,48...|\n",
      "| ham|As per your reque...|[as, per, your, r...|[per, request, me...|(262144,[12650,27...|(262144,[12650,27...|\n",
      "|spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|(262144,[23209,27...|(262144,[23209,27...|\n",
      "|spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|(262144,[1546,219...|(262144,[1546,219...|\n",
      "| ham|I'm gonna be home...|[i, m, gonna, be,...|[m, gonna, home, ...|(262144,[12716,17...|(262144,[12716,17...|\n",
      "|spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|(262144,[7415,256...|(262144,[7415,256...|\n",
      "|spam|URGENT! You have ...|[urgent, you, hav...|[urgent, won, 1, ...|(262144,[4629,232...|(262144,[4629,232...|\n",
      "| ham|I've been searchi...|[i, ve, been, sea...|[ve, searching, r...|(262144,[15585,19...|(262144,[15585,19...|\n",
      "| ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|(262144,[19036,39...|(262144,[19036,39...|\n",
      "|spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(262144,[26364,27...|(262144,[26364,27...|\n",
      "| ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|[oh, k, m, watching]|(262144,[18184,19...|(262144,[18184,19...|\n",
      "| ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(262144,[12524,19...|(262144,[12524,19...|\n",
      "| ham|Fine if thats th...|[fine, if, that, ...|[fine, way, u, fe...|(262144,[48448,51...|(262144,[48448,51...|\n",
      "|spam|England v Macedon...|[england, v, mace...|[england, v, mace...|(262144,[27576,30...|(262144,[27576,30...|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a binary index label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| tag|             message|               words|             removed|         rawFeatures|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(262144,[38555,52...|(262144,[38555,52...|  0.0|\n",
      "| ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|(262144,[16877,51...|(262144,[16877,51...|  0.0|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(262144,[12250,12...|(262144,[12250,12...|  1.0|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(262144,[2306,517...|(262144,[2306,517...|  0.0|\n",
      "| ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|(262144,[19036,25...|(262144,[19036,25...|  0.0|\n",
      "|spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(262144,[19036,19...|(262144,[19036,19...|  1.0|\n",
      "| ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|(262144,[27576,48...|(262144,[27576,48...|  0.0|\n",
      "| ham|As per your reque...|[as, per, your, r...|[per, request, me...|(262144,[12650,27...|(262144,[12650,27...|  0.0|\n",
      "|spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|(262144,[23209,27...|(262144,[23209,27...|  1.0|\n",
      "|spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|(262144,[1546,219...|(262144,[1546,219...|  1.0|\n",
      "| ham|I'm gonna be home...|[i, m, gonna, be,...|[m, gonna, home, ...|(262144,[12716,17...|(262144,[12716,17...|  0.0|\n",
      "|spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|(262144,[7415,256...|(262144,[7415,256...|  1.0|\n",
      "|spam|URGENT! You have ...|[urgent, you, hav...|[urgent, won, 1, ...|(262144,[4629,232...|(262144,[4629,232...|  1.0|\n",
      "| ham|I've been searchi...|[i, ve, been, sea...|[ve, searching, r...|(262144,[15585,19...|(262144,[15585,19...|  0.0|\n",
      "| ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|(262144,[19036,39...|(262144,[19036,39...|  0.0|\n",
      "|spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(262144,[26364,27...|(262144,[26364,27...|  1.0|\n",
      "| ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|[oh, k, m, watching]|(262144,[18184,19...|(262144,[18184,19...|  0.0|\n",
      "| ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(262144,[12524,19...|(262144,[12524,19...|  0.0|\n",
      "| ham|Fine if thats th...|[fine, if, that, ...|[fine, way, u, fe...|(262144,[48448,51...|(262144,[48448,51...|  0.0|\n",
      "|spam|England v Macedon...|[england, v, mace...|[england, v, mace...|(262144,[27576,30...|(262144,[27576,30...|  1.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"tag\", outputCol=\"label\")\n",
    "rescaled_data = indexer.fit(rescaled_data).transform(rescaled_data)\n",
    "\n",
    "rescaled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = rescaled_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size is 3835 and test size is 1739\n"
     ]
    }
   ],
   "source": [
    "print (f\"train size is {train.count()} and test size is {test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes( smoothing=1.0,  modelType='multinomial', featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------+----------+\n",
      "|label|probability                                |prediction|\n",
      "+-----+-------------------------------------------+----------+\n",
      "|0.0  |[1.0,5.502340741671167E-33]                |0.0       |\n",
      "|0.0  |[1.0,3.585780293161731E-21]                |0.0       |\n",
      "|0.0  |[1.0,3.0248056266939986E-84]               |0.0       |\n",
      "|0.0  |[1.0,1.036257089583559E-62]                |0.0       |\n",
      "|0.0  |[1.0,2.3702720436641773E-145]              |0.0       |\n",
      "|0.0  |[1.0,2.0679794336363576E-63]               |0.0       |\n",
      "|0.0  |[1.0,1.1672419820682577E-84]               |0.0       |\n",
      "|0.0  |[1.0,1.127062778635335E-37]                |0.0       |\n",
      "|0.0  |[1.0,2.1757785434878435E-54]               |0.0       |\n",
      "|0.0  |[1.0,6.448160649666164E-112]               |0.0       |\n",
      "|0.0  |[1.0,1.4186556060283386E-50]               |0.0       |\n",
      "|0.0  |[1.0,2.009276465738912E-30]                |0.0       |\n",
      "|0.0  |[1.0,2.1452869468664015E-22]               |0.0       |\n",
      "|0.0  |[0.9999999999999998,1.140129183200513E-16] |0.0       |\n",
      "|0.0  |[0.9999999999999951,4.8818550500000506E-15]|0.0       |\n",
      "|0.0  |[1.0,3.8207240126874107E-97]               |0.0       |\n",
      "|0.0  |[1.0,7.20022966185325E-37]                 |0.0       |\n",
      "|0.0  |[1.0,5.107238775685352E-39]                |0.0       |\n",
      "|0.0  |[1.0,9.860316400447512E-24]                |0.0       |\n",
      "|0.0  |[1.0,8.1245574384080085E-81]               |0.0       |\n",
      "+-----+-------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_result = nb_model.transform(test)\n",
    "test_result.select('label',\n",
    "  'probability',\n",
    " 'prediction').show(truncate =False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_eval = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is computed as 0.9819213845067308 on test set\n"
     ]
    }
   ],
   "source": [
    "f1 = nb_eval.evaluate(test_result)\n",
    "\n",
    "print(f'F1 score is computed as {f1} on test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
