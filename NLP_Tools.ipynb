{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP tools:\n",
    "\n",
    "# Tokenization with regex\n",
    "\n",
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('MLlibnlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of regex patterns in Python:\n",
    "\n",
    "\\w  \"word character\": Unicode letter, ideogram, digit(0-9), or underscore          (e.g., \\w-\\w\\w\\w  matches A-3_b) \n",
    "\n",
    "\\s  \"whitespace character\": any Unicode separator (space, tab, newline,...)   (e.g., a\\sb\\sc  matches a b c) \n",
    "\n",
    "\\d   one Unicode \"digit\" in any script                                          (e.g., file_\\d\\d  matches file_9੩) \n",
    "\n",
    "\\  Escapes a special character     \n",
    "\n",
    "\\*  zero or no time\n",
    "\n",
    "\\+  One or more times\n",
    "\n",
    "\\?  none or one time\n",
    "\n",
    "\n",
    "**NB.** Regex in pyspark internally uses **java regex**. Since backslash have a special meaning in java, we need to escape it with **another backslash**! (e.g., \"\\\\\\s\" instead of \"\\s\")\n",
    "\n",
    "### RegexTokenizer:\n",
    "RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern”(regex) is \"\\\\\\s+\".\n",
    "\n",
    "Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.\n",
    "\n",
    "Check the regex in use here: https://regex101.com/r/2lk6eV/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish not Java c...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish not Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: **bag of words** extraction using RegexTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\w+\", gaps= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+-----------------------------------------------+\n",
      "|id |sentence                              |words                                          |\n",
      "+---+--------------------------------------+-----------------------------------------------+\n",
      "|0  |Hi I heard about Spark                |[hi, i, heard, about, spark]                   |\n",
      "|1  |I wish not Java could use case classes|[i, wish, not, java, could, use, case, classes]|\n",
      "|2  |Logistic,regression,models,are,neat   |[logistic, regression, models, are, neat]      |\n",
      "+---+--------------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "df_tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWordsRemover\n",
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and **don’t carry as much meaning**. Examples are 'the', 'a', 'I', 'had', 'is', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol ='words', outputCol ='removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|             removed|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|  [hi, heard, spark]|\n",
      "|  1|I wish not Java c...|[i, wish, not, ja...|[wish, java, use,...|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(df_tokenized).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB. 'not' has been removed too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "\n",
    "An n-gram is a sequence of n tokens (typically words) for some integer n. The NGram class can be used to transform input features into n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|n-gram                                                                     |\n",
      "+---------------------------------------------------------------------------+\n",
      "|[hi i, i heard, heard about, about spark]                                  |\n",
      "|[i wish, wish not, not java, java could, could use, use case, case classes]|\n",
      "|[logistic regression, regression models, models are, are neat]             |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=2, inputCol='words', outputCol='n-gram')\n",
    "df_ngram = ngram.transform(df_tokenized)\n",
    "df_ngram.select('n-gram').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF refers to the term frequency-inverse document frequency algorithm. \n",
    "\n",
    "TF-IDF in spark has **two phases**. First we should get term frequency (TF) vectors by calling **HashingTF** or **CountVectorizer**. Then, create an **IDF** model.\n",
    "\n",
    "In our example, we have started with a set of sentences (sentenceDataFrame). We have split each sentence into words using Tokenizer (df_tokenized). For each sentence (**bag of words**), we use HashingTF to hash the sentence into a feature vector. We use IDF to rescale the feature vectors; this generally improves performance when using text as features. Our feature vectors could then be passed to a learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get term frequency \n",
    "hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')  # numFeatures=262144 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_data = hashing_tf.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------+\n",
      "|id |rawFeatures                                                                                    |\n",
      "+---+-----------------------------------------------------------------------------------------------+\n",
      "|0  |(262144,[18700,19036,33808,66273,173558],[1.0,1.0,1.0,1.0,1.0])                                |\n",
      "|1  |(262144,[19036,20719,55551,58672,98717,109547,192310,221693],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|2  |(262144,[46243,58267,91006,160975,190884],[1.0,1.0,1.0,1.0,1.0])                               |\n",
      "+---+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.select('id','rawFeatures').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_model = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                                                                                                                                                |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |(262144,[18700,19036,33808,66273,173558],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                                             |\n",
      "|1  |(262144,[19036,20719,55551,58672,98717,109547,192310,221693],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
      "|2  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                                             |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('id','features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB.** In HashingTF, to reduce the chance of **collision**, we can increase the target feature dimension, i.e. the number of **buckets of the hash** table.\n",
    "\n",
    "numFeatures=262144 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
