{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>AWS EC2 PySpark setup</h1>\n",
    "\n",
    "<h2>Free AWS account setup:</h2>\n",
    "\n",
    "AWS free tier account includes 750 hours each month for one year. To stay within the Free Tier, you should use only EC2 Micro instances.\n",
    "<OL>    \n",
    "    <LI>Go to: https://aws.amazon.com/free/ </LI>\n",
    "    <LI>Create Free Acount</LI>\n",
    "    <LI>Enter billing information (Although you are registering for a free account, the billing info is required!)</LI>\n",
    "    <LI>Wait for ID verification code</LI>\n",
    "    <LI>Choose free support plan</LI>\n",
    "</OL>\n",
    "\n",
    "\n",
    "<h2>Create an EC2 instance:</h2> \n",
    "\n",
    "An EC2 instance is a virtual server in the cloud.\n",
    "<OL> \n",
    "    <LI>Log in to your AWS account.</LI>\n",
    "    <LI>Go to Services>Compute>EC2.</LI>\n",
    "    <LI>Click on launch instance.</LI>\n",
    "    <LI>Choose **Ubuntu** from Amazon Machine Image list (make sure to choose the one that is **'Free tier eligible'**.) For example, Ubuntu Server 18.04 LTS (HVM), SSD Volume Type.</LI>\n",
    "    <LI>Choose an instance type. E.g., General purpose t2.micro free tier eligible (1 CPU, 1 GB).</LI>\n",
    "    <LI>Configure instance. Here, you can set the number of instances. Set it to 1 for free account, but in real Spark application we would need more instances for distributed computing.</LI>\n",
    "     <LI>Click on 'Add Storage'. Keep the default, which is 8 GiB</LI>\n",
    "    <LI>Click on 'Add Tags. Set Key as 'myspark' and Value as 'mymachine'.</LI>\n",
    "    <LI>Click on 'Configure Security Group'. Set Type as 'All traffic'. </LI>\n",
    "    <LI>Click on 'Review and Launch'.</LI>\n",
    "    <LI>Click on 'Launch'.</LI>\n",
    "    <LI>**Important** step: In the pop-up window, choose 'create a new key pair'. In Key pair name, write down 'newspark'. Then click 'Download Key Pair'. Once downloaded the newspark.pem file, click on 'Launch instances'.</LI>\n",
    "    <LI>Click on the instance ID</LI>\n",
    "    <LI>To start/stop/reboot the instance: Actions>Instance State></LI>\n",
    "    <LI>To terminate (delete) the instance choose  Actions>Instance State>Terminate</LI>\n",
    "</OL>\n",
    "\n",
    "<h2>Connect to our instance using SSH in Linux terminal. </h2>\n",
    "<OL> \n",
    "    <LI>First, open a terminal in your computer. Then, run the bellow command to make sure the newspark.pem private key file is not publicly viewable:</LI>\n",
    "<pre><code>chmod 400 newspark.pem</code></pre>\n",
    "    <LI>Go to AWS console. Go to your EC2 instance. At the bottom of the page copy the <B>Public DNS</B> address. E.g., ec2-3-23-98-0.us-east-2.compute.amazonaws.com </LI>\n",
    "    <LI>In your computer's terminal, enter the following SSH command (do not forget to add <B>ubuntu@</B> before the Public DNS address). </LI>\n",
    "    <pre><code>ssh -i newspark.pem ubuntu@ec2-3-23-98-0.us-east-2.compute.amazonaws.com</code></pre>\n",
    "    NB. Replace 'ec2-3-23-98-0.us-east-2.compute.amazonaws.com' with your own Public DNS address you got in previous step.\n",
    "    <LI>Now, you have access to the EC2 instance. Type python3. You can run python codes.</LI>\n",
    "    <LI></LI>\n",
    "</OL>\n",
    "\n",
    "<h2>Install the following required packages on EC2 instance</h2>\n",
    "\n",
    "\n",
    "```\n",
    "sudo apt-get update\n",
    "sudo apt install python3-pip\n",
    "pip3 install notebook\n",
    "sudo apt install jupyter-notebook\n",
    "sudo apt-get install default-jre\n",
    "sudo apt-get install scala\n",
    "pip3 install py4j\n",
    "```\n",
    "Now install Hadoop and Spark:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "wget http://archive.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "sudo tar -zxvf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "Connect python with spark:\n",
    "\n",
    "```\n",
    "pip3 install findspark\n",
    "```\n",
    "\n",
    "Congigure jupyter notebook:\n",
    "\n",
    "```\n",
    "jupyter notebook --generate-config\n",
    "```\n",
    "Create a .pem certfication file that we are going to use for our jupyter configuration file. \n",
    "\n",
    "```\n",
    "cd\n",
    "mkdir certs\n",
    "cd certs\n",
    "sudo openssl req -x509 -nodes -days 365 -ne```wkey rsa:1024 -keyout mycert.pem -out mycert.pem\n",
    "```\n",
    "\n",
    "open jupyter\n",
    "```\n",
    "cd ~/.jupyter/\n",
    "vi jupyter_notebook_config.py\n",
    "\n",
    "```\n",
    "Now insert teh following to the above config file:\n",
    "```\n",
    "c = get_config()\n",
    "# Notebook config this is where you saved your pem cert\n",
    "c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem'\n",
    "# listen on all IPs\n",
    "c.NotebookApp.ip = '*'\n",
    "# Don't open browser by default\n",
    "c.NotebookApp.open_browser = False\n",
    "# Fix port to 8888\n",
    "c.NotebookApp.port = 8888\n",
    "```\n",
    "Save it by pressing ESC then typing :wq!\n",
    "Now you are ready to run jupyter notebook.\n",
    "```\n",
    "cd \n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "You are given a URL into the jupyter notebook, copy past the URL into your browser, but replace <I>localhost</I> by your actual EC2 address. That is replace <I>localhost</I> by the <I>Public DNS address</I> of your EC2 instance. E.g.,\n",
    "https://ec2-3-23-98-0.us-east-2.compute.amazonaws.com:8888/?token=15fda6a4f35367ed95cbbdd0300f6d73b608372be0c65cdf\n",
    "\n",
    "You get Not Secure alert. Click on <I>Advanced</I> then <I>proceed</I>.\n",
    "\n",
    "Voila! Jupyter notebook is running on your EC2 and you are able to access it through your computer's browser.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to mport spark you need to use the findspark package.\n",
    "```\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.4.6-bin-hadoop2.7')\n",
    "import pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
